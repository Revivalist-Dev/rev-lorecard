### **System Design: AI Lorebook Generator**

#### **1. Overview**

This document outlines the architecture for an application designed to automate the creation of structured "lorebooks" from web-based sources. The system leverages Large Language Models (LLMs) to intelligently generate search criteria, parse web pages, extract relevant information, and summarize it into a usable format for creative writing or AI roleplaying applications. The core process is a user-guided, four-stage workflow: Search Parameter Generation, Source Discovery & Crawling, Link Confirmation, and Entry Generation.

*   **Language:** Python
*   **Package Manager:** `uv`
*   **API framework:** Litestar
*   **Database:** SQL (SQLite and PostgreSQL). No ORM.
*   **Model validation:** Pydantic
*   **Web Scraping:** `BeautifulSoup4` for HTML parsing.
*   **Template framework:** Jinja for prompt templating.

#### **2. Core Entities & Data Models**

##### **2.1. `Project`**

*   **Purpose**: A `Project` is the primary organizational unit, representing a single lorebook generation task. It encapsulates all configuration, source data, and results for creating one lorebook.

*   **Fields**:
    *   `id` (Text, Primary Key): A unique, human-readable identifier, derived from the project name (e.g., "skyrim-locations").
    *   `name` (Text): The user-friendly display name of the project (e.g., "Skyrim Locations Lorebook").
    *   `prompt` (Text, nullable): The high-level user-provided prompt to generate search parameters (e.g., "Skyrim locations").
    *   `search_params` (JSONB, nullable): Structured search parameters generated by an LLM (`purpose`, `extraction_notes`, `criteria`).
    *   `templates` (JSONB): A structured object containing all Jinja prompt templates for the project's tasks.
        *   `selector_generation` (Text): The prompt used to instruct an LLM to analyze HTML and return CSS selectors for content, categories, and pagination.
        *   `entry_creation` (Text): The prompt used to instruct an LLM to process a scraped web page into a structured lorebook entry.
        *   `search_params_generation` (Text): The prompt used to instruct an LLM to generate search parameters from a user prompt.
    *   `ai_provider_config` (JSONB): A structured object containing all necessary information to interact with the LLM API (e.g., `api_provider`, `model_name`, `model_parameters`).
    *   `requests_per_minute` (Integer): The maximum number of API requests to make per minute for this project, used for rate limiting.
    *   `status` (Enum): The current high-level state of the project (`draft`, `search_params_generated`, `selector_generated`, `links_extracted`, `processing`, `completed`, `failed`).
    *   `created_at` (Timestamp with Timezone): The timestamp when the project was created.
    *   `updated_at` (Timestamp with Timezone): The timestamp when the project was last updated.

---

##### **2.2. `ProjectSource`**

*   **Purpose**: Represents a single web source (e.g., a wiki category page) that can be crawled for links. A `Project` can have multiple `ProjectSource`s, which can be hierarchically related.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique identifier for the source.
    *   `project_id` (Text, Foreign Key): Links the source to its parent `Project`.
    *   `url` (Text): The starting URL for crawling this source.
    *   `link_extraction_selector` (List of Text, nullable): A list of CSS selectors used to identify and extract individual article links.
    *   `link_extraction_pagination_selector` (Text, nullable): A single CSS selector for the 'next page' link to enable crawling multiple pages.
    *   `max_pages_to_crawl` (Integer): The maximum number of pages to follow using the pagination selector for this source.
    *   `max_crawl_depth` (Integer): The maximum depth of sub-categories to discover and crawl starting from this source. A depth of `1` means only the source itself will be processed.
    *   `last_crawled_at` (Timestamp with Timezone, nullable): The timestamp of the last successful crawl.
    *   `created_at` / `updated_at` (Timestamp with Timezone): Timestamps for source lifecycle tracking.

---

##### **2.3. `ProjectSourceHierarchy`**

*   **Purpose**: Stores the hierarchical parent-child relationships between `ProjectSource`s. This enables the system to track the nested structure of discovered sub-categories.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique identifier for the relationship record.
    *   `project_id` (Text, Foreign Key): Links the relationship to the relevant `Project`.
    *   `parent_source_id` (UUID, Foreign Key): The ID of the parent `ProjectSource`.
    *   `child_source_id` (UUID, Foreign Key): The ID of the child `ProjectSource` discovered from the parent.
    *   `created_at` (Timestamp with Timezone): The timestamp when the relationship was established.

---

##### **2.4. `Link`**

*   **Purpose**: Represents a single URL selected by the user for processing. Each `Link` is an atomic unit of work waiting to be processed into a `LorebookEntry`.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique, system-generated identifier for internal use.
    *   `project_id` (Text, Foreign Key): Links the `Link` to its parent `Project`.
    *   `url` (Text): The full, absolute URL of the page to be processed.
    *   `status` (Enum): The current lifecycle state (`pending`, `processing`, `completed`, `failed`, `skipped`).
    *   `error_message` (Text, nullable): Stores the error details if processing fails for this specific link.
    *   `skip_reason` (Text, nullable): Stores the reason provided by the LLM for skipping this link if it's deemed invalid.
    *   `lorebook_entry_id` (UUID, Foreign Key, nullable): A reference to the final `LorebookEntry` that was created from this link.
    *   `raw_content` (Text, nullable): The raw text content scraped from the source URL. Caching this allows for reprocessing with a new prompt without needing to re-scrape the page.
    *   `created_at` (Timestamp with Timezone): The timestamp when the link was extracted.

---

##### **2.5. `LorebookEntry`**

*   **Purpose**: The final, structured output of the generation process. It represents a single, self-contained piece of lore ready for use.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique identifier for the lorebook entry.
    *   `project_id` (Text, Foreign Key): Links the entry to the `Project` it belongs to.
    *   `title` (Text): The main title of the lore entry, as generated by the LLM.
    *   `content` (Text): The summarized body of the lore entry.
    *   `keywords` (List of Text, JSONB): A list of keywords, generated by the LLM, that can be used to trigger this entry in a roleplay application.
    *   `source_url` (Text): The original URL from which the entry's content was derived, for traceability.
    *   `created_at` (Timestamp with Timezone): The timestamp when the entry was created.
    *   `updated_at` (Timestamp with Timezone): The timestamp when the entry was last updated (e.g., by a user).

---

##### **2.6. `BackgroundJob`**

*   **Purpose**: Represents a single asynchronous task executed by a background worker. This table serves as a robust, database-backed task queue.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique identifier for the job.
    *   `task_name` (Text): The name of the task to be executed (e.g., `generate_search_params`, `discover_and_crawl_sources`, `confirm_links`, `process_project_entries`).
    *   `payload` (JSONB, nullable): Additional arguments required for the task.
    *   `status` (Enum): The job's state (`pending`, `in_progress`, `completed`, `failed`, `cancelling`, `canceled`).
    *   `project_id` (Text, Foreign Key, nullable): Links the job to the `Project` it operates on.
    *   `created_at` / `updated_at` (Timestamp with Timezone): Timestamps for job lifecycle tracking.
    *   `result` (JSONB): The result of the job if successful. For `discover_and_crawl_sources`, this includes `new_links`, `existing_links`, `new_sources_created`, and `selectors_generated`.
    *   `error_message` (Text): Error details if the job failed.
    *   `total_items` (Integer, nullable): The total number of items to be processed (e.g., number of sources to crawl).
    *   `processed_items` (Integer, nullable): The number of items processed so far.
    *   `progress` (Real, nullable): The completion percentage (0.0 to 100.0).

---

##### **2.7. `ApiRequestLog`**

*   **Purpose**: An immutable audit record of every billable call made to an external LLM API, essential for cost tracking, performance analysis, and detailed debugging.

*   **Fields**:
    *   `id` (UUID, Primary Key): A unique identifier for the log record.
    *   `project_id` (Text, Foreign Key): Links the request back to the project for cost aggregation.
    *   `job_id` (UUID, Foreign Key, nullable): Links the log to the `BackgroundJob` that initiated it.
    *   `api_provider` (Text): The API provider used (e.g., "openrouter").
    *   `model_used` (Text): The exact model string used for the call.
    *   `request` (JSONB): The full request payload sent to the API.
    *   `response` (JSONB, nullable): The full response payload received from the API.
    *   `input_tokens` / `output_tokens` (Integer, nullable): Token usage metrics.
    *   `calculated_cost` (Numeric, nullable): The monetary cost of the API call.
    *   `latency_ms` (Integer): The duration of the API call in milliseconds.
    *   `timestamp` (Timestamp with Timezone): The exact time the request was made.
    *   `error` (Boolean): A flag indicating if the API call resulted in an error.

---

##### **2.8. `GlobalTemplate`**

*   **Purpose**: Stores reusable Jinja prompt templates that can be referenced by any `Project`. This allows for a centralized library of prompts that can be maintained independently from individual projects.

*   **Fields**:
    *   `id` (Text, Primary Key): A unique, human-readable identifier for the template (e.g., "default-selector-prompt").
    *   `name` (Text, Unique): A user-friendly, unique name for the template (e.g., "Default Selector Prompt").
    *   `content` (Text): The full Jinja template string.
    *   `created_at` / `updated_at` (Timestamp with Timezone): Timestamps for template lifecycle tracking.

---

##### **2.9. Database Schema & Relationships**

*   **Relationships**:
    *   `ProjectSource.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `ProjectSourceHierarchy.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `ProjectSourceHierarchy.parent_source_id` -> `ProjectSource.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `ProjectSourceHierarchy.child_source_id` -> `ProjectSource.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `Link.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `LorebookEntry.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `Link.lorebook_entry_id` -> `LorebookEntry.id` (One-to-One, nullable). `ON DELETE SET NULL`.
    *   `BackgroundJob.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `ApiRequestLog.project_id` -> `Project.id` (Many-to-One). `ON DELETE CASCADE`.
    *   `ApiRequestLog.job_id` -> `BackgroundJob.id` (Many-to-One). `ON DELETE SET NULL`.

*   **Indexing Strategy**:
    *   `Project`: Index on `id` (Primary Key).
    *   `ProjectSource`: Index on `project_id`.
    *   `ProjectSourceHierarchy`: Indexes on `project_id`, `parent_source_id`, `child_source_id`. Unique constraint on `(parent_source_id, child_source_id)`.
    *   `Link`:
        *   Index on `(project_id, status)` for efficient querying of pending links by workers.
        *   Unique constraint on `(project_id, url)` to prevent duplicate link processing.
    *   `LorebookEntry`: Index on `project_id` for efficient retrieval of a project's entries.
    *   `BackgroundJob`: Index on `(status, created_at)` for efficient polling by workers.
    *   `ApiRequestLog`: Index on `project_id` for cost aggregation.
    *   `GlobalTemplate`: Index on `id` (Primary Key), Unique constraint on `name`.

#### **3. Core Workflows**

##### **3.1. Workflow A: Project Setup & Search Parameter Generation**

1.  **Project Creation**: A user creates a new `Project` by providing a `name` and a high-level `prompt`. The user then adds one or more `ProjectSource`s, each with a starting URL. The project's initial `status` is `draft`.
2.  **Job Invocation**: The user initiates the first step via the API, which creates a `BackgroundJob` with `task_name: 'generate_search_params'`.
3.  **Worker Execution**: A background worker picks up the job.
    *   It constructs a prompt using the `templates.search_params_generation` template and the project's `prompt`.
    *   It sends the prompt to the configured LLM, asking for a structured JSON response containing `purpose`, `extraction_notes`, and `criteria`.
    *   It creates an `ApiRequestLog` to record this transaction.
    *   Upon receiving a valid response, the worker updates the `Project.search_params` field and `Project.status` to `search_params_generated`.
    *   Finally, it marks the job as `completed`.

##### **3.2. Workflow B: Source Discovery & Crawling**

1.  **Job Invocation**: The user selects one or more root `ProjectSource`s and initiates this step. This creates a `BackgroundJob` with `task_name: 'discover_and_crawl_sources'`.
2.  **Worker Execution**: A background worker picks up the job and begins a queue-based crawling process.
    *   The worker iterates through the queue of sources to process. Initially, this contains the user-selected sources.
    *   **For each `ProjectSource`**:
        a.  It scrapes the raw HTML from the source's `url`.
        b.  It constructs a prompt using the `templates.selector_generation` template, including the scraped HTML and the project's `search_params`.
        c.  It sends the prompt to the LLM, asking for CSS selectors for three types of links: **content**, **category**, and **pagination**. An `ApiRequestLog` is created.
        d.  The worker updates the `ProjectSource` with the new selectors.
        e.  It then uses these selectors to crawl the source (and subsequent pages if a pagination selector was found).
        f.  **Category Links**: If the current crawl depth is less than the source's `max_crawl_depth`, any discovered category URLs are processed. For each one, the worker creates a new `ProjectSource` (if it doesn't exist), records the parent-child relationship in `ProjectSourceHierarchy`, and adds the new source to the processing queue.
        g.  **Content Links**: All discovered content links are collected.
    *   The job result aggregates all newly found, unique content URLs from the entire crawl process.
    *   The worker updates the `Project.status` to `selector_generated` and marks the job as `completed`.
3.  **User Confirmation**: The frontend displays the newly found URLs, allowing the user to review and deselect any unwanted links before saving them to the project.

##### **3.3. Workflow C: Link Confirmation**

1.  **Job Invocation**: After the user confirms the desired links on the frontend, the client sends this list of URLs to the backend, creating a `BackgroundJob` with `task_name: 'confirm_links'`.
2.  **Worker Execution**: A background worker picks up the job.
    *   It reads the list of URLs from the job's `payload`.
    *   For each URL, it creates a new `Link` record in the database with `status: 'pending'`. It enforces the unique constraint to avoid duplicates.
    *   Upon completion, it updates the `Project.status` to `links_extracted` and marks the job as `completed`.

##### **3.4. Workflow D: Lorebook Entry Generation**

1.  **Job Invocation**: The user starts the main generation process. This creates the primary long-running `BackgroundJob` with `task_name: 'process_project_entries'`.
2.  **Worker Execution**:
    *   A worker picks up the job and sets the `Project.status` to `processing`.
    *   The worker queries the database for all `Link` records with `status: 'pending'` or `status: 'failed'`.
    *   **For each `Link`**:
        a.  It updates the `Link.status` to `processing`.
        b.  It scrapes the content from the `Link.url`, storing the cleaned result in the `Link.raw_content` field.
        c.  It uses the `templates.entry_creation` prompt, providing the `raw_content` to the LLM and requesting a structured JSON response containing `title`, `content`, and `keywords`.
        d.  An `ApiRequestLog` is created for this LLM call.
        e.  On success, it parses the LLM's response, creates a `LorebookEntry` record, and updates the `Link`'s status to `completed` and sets the `lorebook_entry_id`.
        f.  If the LLM deems the content invalid, it updates the `Link` status to `skipped`.
        g.  On failure, it updates the `Link`'s `status` to `failed` and records the `error_message`.
    *   After processing each link, the worker updates the job's progress and checks for cancellation requests.
3.  **Completion**: Once all `Link` records are processed, the worker updates the `Project.status` to `completed` and marks its own job as `completed`.

#### **4. API Endpoints**

*   **Projects**
    *   `POST /projects`: Create a new lorebook project.
    *   `GET /projects`: List all projects.
    *   `GET /projects/{project_id}`: Retrieve a single project's details.
    *   `PATCH /projects/{project_id}`: Update a project.
    *   `DELETE /projects/{project_id}`: Delete a project.
    *   `GET /projects/{project_id}/links`: Get a paginated list of links for the project.
    *   `GET /projects/{project_id}/links/processable-count`: Get the count of pending/failed links.
    *   `GET /projects/{project_id}/entries`: Get a paginated list of generated lorebook entries.
    *   `GET /projects/{project_id}/logs`: Get a paginated list of API request logs for the project.
    *   `GET /projects/{project_id}/lorebook/download`: Get the lorebook in a downloadable format.

*   **Project Sources**
    *   `GET /projects/{project_id}/sources`: List all sources for a project.
    *   `POST /projects/{project_id}/sources`: Add a new source to a project.
    *   `GET /projects/{project_id}/sources/hierarchy`: Get the parent-child relationships between sources for a project.
    *   `PATCH /projects/{project_id}/sources/{source_id}`: Update a source.
    *   `DELETE /projects/{project_id}/sources/{source_id}`: Delete a source.
    *   `POST /projects/{project_id}/sources/delete-bulk`: Delete multiple sources in one request.

*   **Lorebook Entries**
    *   `GET /entries/{entry_id}`: Retrieve a single lorebook entry.
    *   `PUT /entries/{entry_id}`: Manually edit a generated lorebook entry.
    *   `DELETE /entries/{entry_id}`: Delete a lorebook entry.

*   **Background Jobs**
    *   `GET /jobs`: List all background jobs.
    *   `GET /jobs/{job_id}`: Retrieve a single job's status and progress.
    *   `GET /jobs/latest`: Get the latest job for a project by task name.
    *   `POST /jobs/generate-search-params`: Create a job to generate search parameters from a project's prompt (Workflow A).
    *   `POST /jobs/discover-and-crawl`: Create a job to discover sub-categories and crawl sources for links (Workflow B).
    *   `POST /jobs/confirm-links`: Create a job to ingest a user-confirmed list of URLs (Workflow C).
    *   `POST /jobs/process-project-entries`: Create a job to process all pending links into lorebook entries (Workflow D).
    *   `POST /jobs/rescan-links`: Create a job to re-crawl sources using existing selectors without using an LLM.
    *   `POST /jobs/{job_id}/cancel`: Request cancellation of a running job.

*   **Providers**
    *   `GET /providers`: List all available AI providers and their models.

*   **Server-Sent Events (SSE)**
    *   `GET /sse/subscribe/{project_id}`: Subscribe to a stream of real-time events for a project, primarily for job progress updates.

*   **Analytics**
    *   `GET /analytics/projects/{project_id}`: Get aggregated cost and usage statistics for a project.

*   **Global Templates**
    *   `POST /global-templates`: Create a new global template.
    *   `GET /global-templates`: List all global templates.
    *   `GET /global-templates/{template_id}`: Retrieve a single global template.
    *   `PATCH /global-templates/{template_id}`: Update a global template.
    *   `DELETE /global-templates/{template_id}`: Delete a global template.